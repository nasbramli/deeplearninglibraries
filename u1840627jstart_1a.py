# -*- coding: utf-8 -*-
"""U1840627Jstart_1a.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pekFUi2SOOPMDgUaE5XNjjSIasp2Baf6

# Use [markdown](https://www.markdownguide.org/basic-syntax/) to label each (sub)question neatly.

This notebook serves as your report. All your answers should be presented within it.

You can submit multiple notebooks (e.g. 1 notebook per part / question).

Before submission, remember to tidy up the notebook and retain only relevant parts.
"""

from google.colab import drive
drive.mount('/content/drive')

full_path='/content/drive/MyDrive/full.csv'

import time

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

import IPython.display as ipd

from scipy.io import wavfile as wav

from sklearn import preprocessing
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix

import tensorflow.keras as keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import *
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping,ModelCheckpoint,LearningRateScheduler
from keras.datasets import fashion_mnist
from keras.utils import np_utils

import tensorflow as tf
from tensorflow.keras import layers, models, callbacks, optimizers

SEED = 42

import os
os.environ['TF_CUDNN_DETERMINISTIC'] = '1'

import random
random.seed(SEED)

import numpy as np
np.random.seed(SEED)

import tensorflow as tf
tf.random.set_seed(SEED)

"""# Read Data"""

df = pd.read_csv('/content/drive/MyDrive/full.csv')

df.head()

df['label'] = df['filename'].str.split('_').str[-2]

df['label'].value_counts()

"""Split and scale dataset"""

columns_to_drop = ['label','filename']

def split_dataset(df, columns_to_drop, test_size, random_state):
  label_encoder = preprocessing.LabelEncoder()

  df['label'] = label_encoder.fit_transform(df['label'])

  df_train, df_test = train_test_split(df, test_size=test_size, random_state=random_state)

  df_train2 = df_train.drop(columns_to_drop,axis=1)
  y_train2 = df_train['label'].to_numpy()

  df_test2 = df_test.drop(columns_to_drop,axis=1)
  y_test2 = df_test['label'].to_numpy()

  return df_train2, y_train2, df_test2, y_test2

def preprocess_dataset(df_train, df_test):

  standard_scaler = preprocessing.StandardScaler()
  df_train_scaled = standard_scaler.fit_transform(df_train)

  df_test_scaled = standard_scaler.transform(df_test)

  return df_train_scaled, df_test_scaled

X_train, y_train, X_test, y_test = split_dataset(df, columns_to_drop, test_size=0.3, random_state=0) # positive labels being encoded as 1

X_train_scaled, X_test_scaled = preprocess_dataset(X_train, X_test)

X_train_scaled.shape

def build_model(batch_size):
  ffn = models.Sequential()
  ffn.add(layers.Dense(units=128, activation='relu', input_shape=(77,)))
  ffn.add(layers.Dense(units=128, activation='relu'))
  ffn.add(layers.Dense(units=128, activation='relu'))
  ffn.add(layers.Dense(units=1, activation='sigmoid'))
  opt=optimizers.Adam()
  ffn.compile(
      optimizer=opt,
      loss='BinaryCrossentropy',
      metrics=['accuracy']
  )

  return ffn

ffn=build_model(3)
ffn.build()
ffn.summary()

from sklearn.model_selection import KFold
from keras.callbacks import Callback
from timeit import default_timer as timer

class TimingCallback(Callback):
  def __init__(self, logs={}):
    self.logs=[]
  def on_epoch_begin(self, epoch, logs={}):
    self.starttime = timer()
  def on_epoch_end(self, epoch, logs={}):
    self.logs.append(timer()-self.starttime)

time = TimingCallback()

batch_size1b=256
accuracy_dict1b={}
time_dict1b={}
no_epochs=100

"""# **Qn 1**
a. Use the training dataset to train the model for 100 epochs. Implement early stopping with patience 3.
"""

accuracies1b=[]
training_times1b=[]

stop_early = callbacks.EarlyStopping(
                            monitor='val_loss', patience=3)
checkpoint = callbacks.ModelCheckpoint(
                        'fnn_model.hdf5',
                        verbose=1,
                        save_best_only=True,
                        monitor='val_loss')
X_train, X_test = X_train_scaled, X_train_scaled
Y_train, Y_test = y_train, y_train
main_model = build_model(batch_size1b)
main_model.summary()
hist = main_model.fit(x=X_train,
                              y=Y_train,
                              validation_data=(X_test, Y_test),
                              epochs=no_epochs,
                              verbose=0,
                              callbacks=[stop_early, checkpoint, time])

accuracies1b.append(hist.history['accuracy'][-1])
training_times1b.append(time.logs[-1])
tf.keras.backend.clear_session()

accuracy_dict1b[f'{batch_size1b}'] = accuracies1b
time_dict1b[f'{batch_size1b}'] = training_times1b

accuracy_dict1b

time_dict1b

"""# **Qn 1**
b. Plot train and test accuracies and losses on training and test data against training epochs and comment on the line plots. Explain the use of early stopping in this question.
"""

hist.history['accuracy']
hist.history['val_accuracy']
hist.history['loss']
hist.history['val_loss']

plt.plot(hist.history['accuracy'])
plt.plot(hist.history['val_accuracy'])
plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('Q1bgraph')
plt.show()

"""Early stopping is generally used to avoid over-fitting when the training model with iterative model is used, such as the adam optimiser for this method, and it would cause the model to have poor perfromance on the test set.

# **Qn 2**
a. Plot mean cross-validation accuracies on the final epoch for different batch sizes as a scatter plot. Limit search space to batch size [128, 256, 512, 1024].
"""

batch_size_list2a=[128,256, 512,1024]
accuracy_dict2a={}
time_dict2a={}

for batch_size in batch_size_list2a:
    kfold = KFold(n_splits=5)
    accuracies2a = []
    training_times2a = []
    for train_index, test_index in kfold.split(X_train_scaled):
        stop_early = callbacks.EarlyStopping(
                            monitor='val_loss', patience=3)
        checkpoint = callbacks.ModelCheckpoint(
                        'fnn_model.hdf5',
                        verbose=1,
                        save_best_only=True,
                        monitor='val_loss')
        X_train, X_test = X_train_scaled[train_index], X_train_scaled[test_index]
        Y_train, Y_test = y_train[train_index], y_train[test_index]
        main_model = build_model(batch_size)
        main_model.summary()
        hist = main_model.fit(x=X_train,
                              y=Y_train,
                              validation_data=(X_test, Y_test),
                              epochs=100,
                              verbose=0,
                              callbacks=[stop_early, checkpoint, time])

        accuracies2a.append(hist.history['accuracy'][-1])
        training_times2a.append(time.logs[-1])
        tf.keras.backend.clear_session()
    accuracy_dict2a[f'{batch_size}'] = accuracies2a
    time_dict2a[f'{batch_size}'] = training_times2a

"""# **Qn 2**
b. Create a table of time taken to train the network on the last epoch against different batch sizes.
"""

time_dict2a

"""# **Qn 2**
c. Select the optimal batch size and state a reason for your selection.
"""

accuracy_dict2a

"""Batch size = 128, would be the optimal batch size, because it gives the highest accuracy.

# **Qn 2**
d. What happens when batch sizes increases & why?

The accuracy increases as the batch size increase, because, bigger batch size has less noise, and it offers better convergence as compared to a smaller batch size, but it is slower.

# **Qn 2**
e. Plot the train & test accuracies against epoch for the optimal batch size in a line plot.
"""

optimal_bs=[128, 256, 512, 1024]
accuracies2e=[0.7413605451583862, 0.7459558248519897, 0.7445743083953857, 0.7578334808349609]
plt.plot(optimal_bs, accuracies2e)
plt.title('Qn2e')
plt.show

"""# **Qn 3**
a. Plot the mean cross-validation accuracies on the final epoch for different numbers of hidden-layer usin g a scatter plot. Limit the search space of the number of neurons to {64, 128, 256}.

Continue using the 5-fold cross validation on the training set.
"""

def build_model(hidden_layer):
  ffn = models.Sequential()
  ffn.add(layers.Dense(units=hidden_layer, activation='relu', input_shape=(77,)))
  ffn.add(layers.Dense(units=hidden_layer, activation='relu'))
  ffn.add(layers.Dense(units=hidden_layer, activation='relu'))
  ffn.add(layers.Dense(units=1, activation='sigmoid'))
  opt=optimizers.Adam()
  ffn.compile(
      optimizer=opt,
      loss='BinaryCrossentropy',
      metrics=['accuracy']
  )

  return ffn

hidden_layer_list3a=[64, 128, 256]
accuracy_dict3a={}
time_dict3a={}

for hidden_layer in hidden_layer_list3a:
    kfold = KFold(n_splits=5)
    accuracies3a = []
    training_times3a = []
    for train_index, test_index in kfold.split(X_train_scaled):
        stop_early = callbacks.EarlyStopping(
                            monitor='val_loss', patience=3)
        checkpoint = callbacks.ModelCheckpoint(
                        'fnn_model.hdf5',
                        verbose=1,
                        save_best_only=True,
                        monitor='val_loss')
        X_train, X_test = X_train_scaled[train_index], X_train_scaled[test_index]
        Y_train, Y_test = y_train[train_index], y_train[test_index]
        main_model = build_model(hidden_layer)
        main_model.summary()
        hist = main_model.fit(x=X_train,
                              y=Y_train,
                              validation_data=(X_test, Y_test),
                              epochs=100,
                              verbose=0,
                              callbacks=[stop_early, checkpoint, time])

        accuracies3a.append(hist.history['accuracy'][-1])
        training_times3a.append(time.logs[-1])
        tf.keras.backend.clear_session()
    accuracy_dict3a[f'{hidden_layer}'] = accuracies3a
    time_dict3a[f'{hidden_layer}'] = training_times3a

"""# **Qn 3**
b. Select the optimal number of neuron for the hidden layer. State the rationale for your selection.
"""

accuracy_dict3a

time_dict3a

"""256 would the most the most optimal number of neurons for the hidden layer, because even tough 128 has the highest accuracy, but if we factor in the time taken as well as accuracy, it would be more optimal to select 256.

# **Qn 3**
c. Plot the train and test accuracies against the training epochs with the optimal number of neurons using a line plot.
"""

plt.plot(hist.history['accuracy'])
plt.plot(hist.history['val_accuracy'])

"""# **Qn 3**
d. How does the dropout work, and what is the purpose of dropouts?

Dropout is a regularization technique for neural network models where randomly selected neurons are ignored during the training, thus they are 'dropped out' randomly.

Thus, making their contribution to the activation of downstream neurons is temporarily removed on the forward pass, and any weight updates aren't applied on the backward pass.

Then, as the neural network learns, neuron weights settle into their context within the network. Weights od neurons are tuned for specific features, providing some specialisation.

After which, the neighbouring neurons come to rely on this specialisaton, which if taken too far, can result in a fragile model too specialise for the training data.

The effect is that the network becomes less sensitive to the specific weights of the neurons. This would then result ina network capable of better generalisation and less likely to overfit the training data.

# **Qn 3**
e. Besides early stopping and dropout, what is another approach that you could take to address overfitting in the model, and how does it work?

Implement the approach.

Ans: we can either use Regularisation or Normalisation.

Here we implement Normalisation.
"""

def build_model(hidden_layer):
  ffn = models.Sequential()
  ffn.add(layers.Dense(units=hidden_layer, activation='relu', input_shape=(77,)))
  ffn.add(layers.BatchNormalization())
  ffn.add(layers.Dense(units=hidden_layer, activation='relu'))
  ffn.add(layers.Dense(units=hidden_layer, activation='relu'))
  ffn.add(layers.Dense(units=1, activation='sigmoid'))
  opt=optimizers.Adam()
  ffn.compile(
      optimizer=opt,
      loss='BinaryCrossentropy',
      metrics=['accuracy']
  )

  return ffn

"""# **Qn 4**
a. Record yourself with a wav file for 5 seconds, either in a positive or negative manner. Preprocess the data using the preprocess script and prepare the data set.
"""

import librosa
import soundfile as sf

import numpy as np
import pandas as pd

import os
from os import listdir
from os.path import isfile, join

from collections import OrderedDict

import json

def extract_features(filepath):

    '''
    Source: https://github.com/danz1ka19/Music-Emotion-Recognition/blob/master/Feature-Extraction.py
    Modified to process a single file

        function: extract_features
        input: path to mp3 files
        output: csv file containing features extracted

        This function reads the content in a directory and for each audio file detected
        reads the file and extracts relevant features using librosa library for audio
        signal processing
    '''

    feature_set = {}  # Features

    # Reading audio file
    y, sr = librosa.load(filepath)
    S = np.abs(librosa.stft(y, n_fft=512))
    # https://librosa.org/doc/main/generated/librosa.stft.html (set 512 for speech processing)

    # Extracting Features
    tempo, beats = librosa.beat.beat_track(y=y, sr=sr)
    chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr, n_fft=512)

    chroma_cq = librosa.feature.chroma_cqt(y=y, sr=sr)

    chroma_cens = librosa.feature.chroma_cens(y=y, sr=sr)
    melspectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=512)
    rmse = librosa.feature.rms(y=y)[0]
    cent = librosa.feature.spectral_centroid(y=y, sr=sr, n_fft=512)
    spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr, n_fft=512)
    contrast = librosa.feature.spectral_contrast(S=S, sr=sr, n_fft=512)
    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr, n_fft=512)
    poly_features = librosa.feature.poly_features(S=S, sr=sr, n_fft=512)

    tonnetz = librosa.feature.tonnetz(y=y, sr=sr)

    zcr = librosa.feature.zero_crossing_rate(y)
    harmonic = librosa.effects.harmonic(y)
    percussive = librosa.effects.percussive(y)

    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_fft=512)
    mfcc_delta = librosa.feature.delta(mfcc)

    onset_frames = librosa.onset.onset_detect(y=y, sr=sr)
    frames_to_time = librosa.frames_to_time(onset_frames[:20], sr=sr)

    # Concatenating Features into one csv and json format
    feature_set['filename'] = filepath  # song name
    feature_set['tempo'] = tempo  # tempo
    feature_set['total_beats'] = sum(beats)  # beats
    feature_set['average_beats'] = np.average(beats)
    feature_set['chroma_stft_mean'] = np.mean(chroma_stft)  # chroma stft
    feature_set['chroma_stft_var'] = np.var(chroma_stft)

    feature_set['chroma_cq_mean'] = np.mean(chroma_cq)  # chroma cq
    feature_set['chroma_cq_var'] = np.var(chroma_cq)

    feature_set['chroma_cens_mean'] = np.mean(chroma_cens)  # chroma cens
    feature_set['chroma_cens_var'] = np.var(chroma_cens)
    feature_set['melspectrogram_mean'] = np.mean(melspectrogram)  # melspectrogram
    feature_set['melspectrogram_var'] = np.var(melspectrogram)
    feature_set['mfcc_mean'] = np.mean(mfcc)  # mfcc
    feature_set['mfcc_var'] = np.var(mfcc)
    feature_set['mfcc_delta_mean'] = np.mean(mfcc_delta)  # mfcc delta
    feature_set['mfcc_delta_var'] = np.var(mfcc_delta)
    feature_set['rmse_mean'] = np.mean(rmse)  # rmse
    feature_set['rmse_var'] = np.var(rmse)
    feature_set['cent_mean'] = np.mean(cent)  # cent
    feature_set['cent_var'] = np.var(cent)
    feature_set['spec_bw_mean'] = np.mean(spec_bw)  # spectral bandwidth
    feature_set['spec_bw_var'] = np.var(spec_bw)
    feature_set['contrast_mean'] = np.mean(contrast)  # contrast
    feature_set['contrast_var'] = np.var(contrast)
    feature_set['rolloff_mean'] = np.mean(rolloff)  # rolloff
    feature_set['rolloff_var'] = np.mean(rolloff)
    feature_set['poly_mean'] = np.mean(poly_features)  # poly features
    feature_set['poly_var'] = np.mean(poly_features)

    feature_set['tonnetz_mean'] = np.mean(tonnetz)  # tonnetz
    feature_set['tonnetz_var'] = np.var(tonnetz)

    feature_set['zcr_mean'] = np.mean(zcr)  # zero crossing rate
    feature_set['zcr_var'] = np.var(zcr)
    feature_set['harm_mean'] = np.mean(harmonic)  # harmonic
    feature_set['harm_var'] = np.var(harmonic)
    feature_set['perc_mean'] = np.mean(percussive)  # percussive
    feature_set['perc_var'] = np.var(percussive)
    feature_set['frame_mean'] = np.mean(frames_to_time)  # frames
    feature_set['frame_var'] = np.var(frames_to_time)

    for ix, coeff in enumerate(mfcc):
        feature_set['mfcc' + str(ix) + '_mean'] = coeff.mean()
        feature_set['mfcc' + str(ix) + '_var'] = coeff.var()

    return feature_set

new_features_dict = extract_features('/content/drive/MyDrive/Introduction.wav')
df1 = pd.DataFrame([new_features_dict])
df1.to_csv('./new_record.csv', index=False)

"""# **Qn 4**
b. Do a model prediction on your sample test dataset and obtain the predicted label using a threshold of 0.5. The model used is the optimised pretrained model using the selected optimal batch size and optimal number of neurons.
"""

best_model=models.load_model('/content/fnn_model.hdf5')

df1.drop(labels='filename', axis = 1, inplace=True)

df1.head()

predictions = best_model.predict(df1)

predictions

"""# **Qn 4**
c. Find the most important features on the model prediction for your test sample using SHAP. Plot the local feature importance with a force plot and explain your
observations
"""

!pip install shap

import shap

import keras.backend
shap.explainers._deep.deep_tf.op_handlers['AddV2'] = shap.explainers._deep.deep_tf.passthrough

shap_exp = shap.DeepExplainer(best_model, X_train_scaled)

shap_values = shap_exp.shap_values(X_test_scaled)

shap.summary_plot(shap_values[0], plot_type='bar', feature_names=df1.columns)

shap.decision_plot(shap_exp.expected_value[0].numpy(), shap_values[0][0], features = df1.iloc[0,:], feature_names = X_test.columns.tolist())

shap.plots._waterfall.waterfall_legacy(shap_exp.expected_value[0].numpy(), shap_values[0][0], feature_names = X_test.columns)