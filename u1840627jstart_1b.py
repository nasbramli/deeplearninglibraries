# -*- coding: utf-8 -*-
"""U1840627Jstart_1b.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UDfoWwi9wKrtlTxfkiAqV0H9SfyDuiXJ
"""

from google.colab import drive
drive.mount('/content/drive')

full_pathb='/content/drive/MyDrive/hdb_price_prediction.csv'

# Setting the seed here is sufficient.
# If you don't plan to use these starter code, make sure you add this cell.

SEED = 42

import os
os.environ['TF_CUDNN_DETERMINISTIC'] = '1'

import random
random.seed(SEED)

import numpy as np
np.random.seed(SEED)

import tensorflow as tf
tf.random.set_seed(SEED)

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import Normalization, StringLookup, IntegerLookup

import pandas as pd
df = pd.read_csv('/content/drive/MyDrive/hdb_price_prediction.csv')
df

len(df['storey_range'].unique())

# The functions in this cell are adapted from https://keras.io/examples/structured_data/structured_data_classification_from_scratch/
# It is the same link as the one mentioned in the question paper (Q1b)

def dataframe_to_dataset(dataframe):
    dataframe = dataframe.copy()
    labels = dataframe.pop("resale_price")
    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
    ds = ds.shuffle(buffer_size=len(dataframe))
    return ds


def encode_numerical_feature(feature, name, dataset):
    # Create a Normalization layer for our feature
    normalizer = Normalization()

    # Prepare a Dataset that only yields our feature
    feature_ds = dataset.map(lambda x, y: x[name])
    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))

    # Learn the statistics of the data
    normalizer.adapt(feature_ds)

    # Normalize the input feature
    encoded_feature = normalizer(feature)
    return encoded_feature


def encode_categorical_feature(feature, name, dataset, is_string):
    lookup_class = StringLookup if is_string else IntegerLookup
    # Create a lookup layer which will turn strings into integer indices
    lookup = lookup_class(output_mode="binary") # NOTE: as mentioned in the question paper, this actually does one-hot encoding. You could replace 'binary' with 'one_hot' if you wish to.

    # Prepare a Dataset that only yields our feature
    feature_ds = dataset.map(lambda x, y: x[name])
    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))

    # Learn the set of possible string values and assign them a fixed integer index
    lookup.adapt(feature_ds)

    # Turn the string input into integer indices
    encoded_feature = lookup(feature)
    return encoded_feature

from keras import backend as K

def r2(y_true, y_pred):
    '''
    # Obtained from https://jmlb.github.io/ml/2017/03/20/CoeffDetermination_CustomMetric4Keras/
    # TODO: you have to find out how to use it in your code
    '''
    SS_res = K.sum(K.square( y_true - y_pred ))
    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )
    return ( 1 - SS_res/(SS_tot + K.epsilon()) )

"""# **Qn 1**
a. Divide dataset into train and test sets by using entries from year 2020 & before as training data.

Why is this done instead of using random train/test splits?
"""

#Creating the Dataset for training and testing
temp_df = df.copy()
temp_df_train = temp_df[temp_df['year'] <= 2020].copy()
temp_df_val = temp_df[temp_df['year'] >= 2021].copy()
temp_df_train = temp_df_train[['dist_to_nearest_stn', 'dist_to_dhoby',  'degree_centrality', 'eigenvector_centrality',
                  'remaining_lease_years', 'floor_area_sqm', 'month', 'flat_model_type', 'storey_range', 'resale_price']]
temp_df_val = temp_df_val[['dist_to_nearest_stn', 'dist_to_dhoby',  'degree_centrality', 'eigenvector_centrality',
                  'remaining_lease_years', 'floor_area_sqm', 'month', 'flat_model_type', 'storey_range', 'resale_price']]

"""It is done because of relevance according to length of time. Patterns in previous years would be a better learning tools for the nerual network than random split of train/tests."""

# Split data

train_dataframe = temp_df_train # TODO
val_dataframe = temp_df_val # TODO

train_ds = dataframe_to_dataset(train_dataframe)
val_ds = dataframe_to_dataset(val_dataframe)

train_ds = train_ds.batch(256)
val_ds = val_ds.batch(256)

"""# **Qn 1**
b. A team of data scientist implimented a linear regression model vila Scikit-learn. Obtained a test of R-squared value of 0.627.

Implement this neural network by following tutorial from Keras documentation. Architecture should resemble figure shown in Appendix A.
"""

# Categorical features encoded as integers
month = keras.Input(shape=(1,), name="month", dtype="int64")

# Categorical feature encoded as string
flat_model_type = keras.Input(shape=(1,), name="flat_model_type", dtype="string")
storey_range = keras.Input(shape=(1,), name="storey_range", dtype="string")

# Numerical features
dist_to_nearest_stn = keras.Input(shape=(1,), name="dist_to_nearest_stn")
dist_to_dhoby = keras.Input(shape=(1,), name="dist_to_dhoby")
degree_centrality = keras.Input(shape=(1,), name="degree_centrality")
eigenvector_centrality = keras.Input(shape=(1,), name="eigenvector_centrality")
remaining_lease_years = keras.Input(shape=(1,), name="remaining_lease_years")
floor_area_sqm = keras.Input(shape=(1,), name="floor_area_sqm")

all_inputs = [
    month,
    flat_model_type,
    storey_range,
    floor_area_sqm,
    remaining_lease_years,
    degree_centrality,
    eigenvector_centrality,
    dist_to_nearest_stn,
    dist_to_dhoby
]

# Integer categorical features
month_encoded = encode_categorical_feature(month, "month", train_ds, False)

# String categorical features
flat_model_type_encoded = encode_categorical_feature(flat_model_type, "flat_model_type", train_ds, True)
storey_range_encoded = encode_categorical_feature(storey_range, "storey_range", train_ds, True)

# Numerical features
dist_to_nearest_stn_encoded = encode_numerical_feature(dist_to_nearest_stn, "dist_to_nearest_stn", train_ds)
dist_to_dhoby_encoded = encode_numerical_feature(dist_to_dhoby, "dist_to_dhoby", train_ds)
degree_centrality_encoded = encode_numerical_feature(degree_centrality, "degree_centrality", train_ds)
eigenvector_centrality_encoded = encode_numerical_feature(eigenvector_centrality, "eigenvector_centrality", train_ds)
remaining_lease_years = encode_numerical_feature(remaining_lease_years, "remaining_lease_years", train_ds)
floor_area_sqm_encoded = encode_numerical_feature(floor_area_sqm, "floor_area_sqm", train_ds)

all_features = layers.concatenate(
    [
        month_encoded,
        flat_model_type_encoded,
        storey_range_encoded,
        floor_area_sqm_encoded,
        remaining_lease_years,
        degree_centrality_encoded,
        eigenvector_centrality_encoded,
        dist_to_nearest_stn_encoded,
        dist_to_dhoby_encoded]
)

"""# **Qn 1**
c. The team suggests to train the model for 50 epochs using mini-batch gradient descent with batch size 256, Adam optimiser and m.s.e as cost function. However, you find that your results are far off from model.

Change to SGD and observe how problem is fixed.

Report the test R-squared value and explain why the change to SGD fixes the problem faced in Adam optimiser.
"""

#Compiling the Model and defining the optimizer, metrics, and loss
output = layers.Dense(1)(all_features)
model_adam = keras.Model(all_inputs, output)
opt = keras.optimizers.Adam()
model_adam.compile(opt, "mean_squared_error", metrics=[r2])

#fitting the model with the training data and checking the loss/accuracy with testing data
model.fit(train_ds, epochs=50, validation_data=val_ds)

"""Test Data R-squared Value for the above model with Adam Optmizer is -10.0767, the Best model had R^2 value of -10.0644 (will vary do check and update if needed)

Changing the Optimizer to SGD
"""

#compiling the model
output = layers.Dense(1)(all_features)
model = keras.Model(all_inputs, output)
model.compile("SGD", "mean_squared_error", metrics=[r2])

#fitting the model
model.fit(train_ds, epochs=50, validation_data=val_ds)

"""Test Data R-squared Value for the above model with SGD Optmizer is 0.6123

By implementation, SGD should be able to generalize better (i.e. perform better on test data) than Adam, while Adam optmizer is combination of RMSprop and AdaGrad, so Adam will be able to take less steps than SGD to find the min loss for the given model.
Given case Adam optimzer learning rate is too low for this to converge to a minimum loss.

# **Qn 1**
d. Add 1 hidden layer(10 units) to the architecture in Qn 1c  and train it with the same config as in Qn 1c excepth that the learning rate is increased to 0.008. Report R-squared value.
"""

#compiling the model
x = layers.Dense(10, activation='relu')(all_features)
output = layers.Dense(1)(x)
model = keras.Model(all_inputs, output)
opt = keras.optimizers.Adam(0.08)
model.compile(opt, "mean_squared_error", metrics=[r2])

#getting a summary of the model to visualizer the model
model.summary()

#fitting the model
model.fit(train_ds, epochs=50, validation_data=val_ds)

"""The best R-squared value is 0.626

# **Qn 1**
e. Cimpare the performance of the linear regression model to the Dense layer and the NN architecture and suggest reasons for the observations you made.

On comparison to linear model, This model similarly, which is visible from the  very small difference in R-squared value.

# **Qn 2**
a. Further split the data from the year 2020 and before by using data from year 2020 as validation set and the rest as the training set.
"""

#Creating a new dataset for question 2
q2 = df.copy()
q2_train = q2[q2['year'] < 2020]
q2_val = q2[q2['year'] == 2020]
q2_train = q2_train[['dist_to_nearest_stn', 'dist_to_dhoby',  'degree_centrality', 'eigenvector_centrality',
                  'remaining_lease_years', 'floor_area_sqm', 'month', 'flat_model_type', 'storey_range', 'resale_price']]
q2_val = q2_val[['dist_to_nearest_stn', 'dist_to_dhoby',  'degree_centrality', 'eigenvector_centrality',
                  'remaining_lease_years', 'floor_area_sqm', 'month', 'flat_model_type', 'storey_range', 'resale_price']]

#Defining the new functions to match requirements of the question
def dataframe_to_dataset(dataframe):
    dataframe = dataframe.copy()
    labels = dataframe.pop("resale_price")
    labels =tf.cast(labels,tf.int32)
    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
    ds = ds.shuffle(buffer_size=len(dataframe))
    return ds

def encode_categorical_feature2(feature, name, dataset, is_string):
    lookup_class = StringLookup if is_string else IntegerLookup
    # Create a lookup layer which will turn strings into integer indices
    lookup = lookup_class(output_mode="one_hot") # NOTE: as mentioned in the question paper, this actually does one-hot encoding. You could replace 'binary' with 'one_hot' if you wish to.

    # Prepare a Dataset that only yields our feature
    feature_ds = dataset.map(lambda x, y: x[name])
    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))

    # Learn the set of possible string values and assign them a fixed integer index
    lookup.adapt(feature_ds)

    # Turn the string input into integer indices
    encoded_feature = lookup(feature)
    return encoded_feature

# Split data

train_dataframe = q2_train # TODO
val_dataframe = q2_val # TODO

train_ds = dataframe_to_dataset(train_dataframe)
val_ds = dataframe_to_dataset(val_dataframe)

train_ds = train_ds.batch(256)
val_ds = val_ds.batch(256)

"""# **Qn 2**
b. For each categorical variable, replace the one-hot encodinf with the layer tf.keras.Embedding(). Set output_dim floor...

The embedding layer produces a 2D output, which cannot be concatenated with other features. Add a Flatten layer to resolve this.
"""

#importing the required libraries
from math import floor
import keras_tuner
from keras import callbacks

#Defininf the model using function for the purpose of hyperparameter tuning
def q2_build_model(hp):

    # Categorical features encoded as integers
    month = keras.Input(shape=(1,), name="month", dtype="int64")

    # Categorical feature encoded as string
    flat_model_type = keras.Input(shape=(1,), name="flat_model_type", dtype="string")
    storey_range = keras.Input(shape=(1,), name="storey_range", dtype="string")

    # Numerical features
    dist_to_nearest_stn = keras.Input(shape=(1,), name="dist_to_nearest_stn")
    dist_to_dhoby = keras.Input(shape=(1,), name="dist_to_dhoby")
    degree_centrality = keras.Input(shape=(1,), name="degree_centrality")
    eigenvector_centrality = keras.Input(shape=(1,), name="eigenvector_centrality")
    remaining_lease_years = keras.Input(shape=(1,), name="remaining_lease_years")
    floor_area_sqm = keras.Input(shape=(1,), name="floor_area_sqm")

    all_inputs = [
        month,
        flat_model_type,
        storey_range,
        floor_area_sqm,
        remaining_lease_years,
        degree_centrality,
        eigenvector_centrality,
        dist_to_nearest_stn,
        dist_to_dhoby
    ]

    # Integer categorical features
    month_encoded = encode_categorical_feature2(month, "month", train_ds, False)
    month_embedded = layers.Embedding(13,output_dim = int(12/hp.Int('divisor',min_value=1,max_value=2,step=1)))(month_encoded)
    month_embedded = layers.Flatten()(month_embedded)

    # String categorical features
    flat_model_type_encoded = encode_categorical_feature2(flat_model_type, "flat_model_type", train_ds, True)
    flat_model_type_embedded = layers.Embedding(45,output_dim = floor(44//hp.Int('divisor',min_value=1,max_value=2,step=1)))(flat_model_type_encoded)
    flat_model_type_embedded = layers.Flatten()(flat_model_type_embedded)
    storey_range_encoded = encode_categorical_feature2(storey_range, "storey_range", train_ds, True)
    storey_range_embedded = layers.Embedding(18,output_dim = floor(17//hp.Int('divisor',min_value=1,max_value=2,step=1)))(storey_range_encoded)
    storey_range_embedded = layers.Flatten()(storey_range_embedded)

    # Numerical features
    dist_to_nearest_stn_encoded = encode_numerical_feature(dist_to_nearest_stn, "dist_to_nearest_stn", train_ds)
    dist_to_dhoby_encoded = encode_numerical_feature(dist_to_dhoby, "dist_to_dhoby", train_ds)
    degree_centrality_encoded = encode_numerical_feature(degree_centrality, "degree_centrality", train_ds)
    eigenvector_centrality_encoded = encode_numerical_feature(eigenvector_centrality, "eigenvector_centrality", train_ds)
    remaining_lease_years = encode_numerical_feature(remaining_lease_years, "remaining_lease_years", train_ds)
    floor_area_sqm_encoded = encode_numerical_feature(floor_area_sqm, "floor_area_sqm", train_ds)

    all_features = layers.concatenate(
        [
            month_encoded,
            #month_embedded,
            #flat_model_type_encoded,
            flat_model_type_embedded,
            #storey_range_encoded,
            storey_range_embedded,
            floor_area_sqm_encoded,
            remaining_lease_years,
            degree_centrality_encoded,
            eigenvector_centrality_encoded,
            dist_to_nearest_stn_encoded,
            dist_to_dhoby_encoded]
    )
    #emd = layers.Embedding(10,output_dim = floor(9/hp.Int('divisor',min_value=1,max_value=2,step=1)))(all_features)

    x = layers.Dense(units=hp.Int('num_of_neurons',min_value=32,max_value=512,step=32),activation='relu')(all_features)
    output = layers.Dense(1)(x)
    model = keras.Model(all_inputs, output)
    learning_rate = hp.Float("lr", min_value=1e-4, max_value=2e-1, sampling="log")
    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss="mean_squared_error", metrics=[r2])

    return model

"""# **Qn 2**
c. Via vallback, introduce early stopping to model.

Use this as a reference, use KerasTuner to tune the model on the validation set according to the following ranges...

Run 10 iterations of parameter search, each for 50 epochs and report the best set of hyperparameters.
"""

#Defining the stop ealry callback
stop_early = callbacks.EarlyStopping(
                            monitor='val_r2', patience=10)

#Defining the optimizer and keras tuner for HP tuning
obj = keras_tuner.Objective("val_r2", direction="max")
tuner2 = keras_tuner.RandomSearch(
    q2_build_model,
    objective=obj,
    max_trials=10)

#Searching the best possible parameters for the model based on random search
tuner2.search(train_ds, epochs=50, validation_data=val_ds, callbacks=[stop_early])

#Getting the best model from the search/group of models
best_model = tuner2.get_best_models()[0]

#Summanry of the top 10 peforming models and the Hyper parameters used
tuner2.results_summary()

#Getting the best hyper parameters
best_hp = tuner2.get_best_hyperparameters()[0]

#Best Learning Rate
print(best_hp['lr'])

#Best Divisor
print(best_hp['divisor'])

#Best Units for Dense layer
print(best_hp['num_of_neurons'])

#Defining the model based on the best hyper parameters
def q2d_build_model():

    # Categorical features encoded as integers
    month = keras.Input(shape=(1,), name="month", dtype="int64")

    # Categorical feature encoded as string
    flat_model_type = keras.Input(shape=(1,), name="flat_model_type", dtype="string")
    storey_range = keras.Input(shape=(1,), name="storey_range", dtype="string")

    # Numerical features
    dist_to_nearest_stn = keras.Input(shape=(1,), name="dist_to_nearest_stn")
    dist_to_dhoby = keras.Input(shape=(1,), name="dist_to_dhoby")
    degree_centrality = keras.Input(shape=(1,), name="degree_centrality")
    eigenvector_centrality = keras.Input(shape=(1,), name="eigenvector_centrality")
    remaining_lease_years = keras.Input(shape=(1,), name="remaining_lease_years")
    floor_area_sqm = keras.Input(shape=(1,), name="floor_area_sqm")

    all_inputs = [
        month,
        flat_model_type,
        storey_range,
        floor_area_sqm,
        remaining_lease_years,
        degree_centrality,
        eigenvector_centrality,
        dist_to_nearest_stn,
        dist_to_dhoby
    ]

    # Integer categorical features
    month_encoded = encode_categorical_feature2(month, "month", train_ds, False)
    month_embedded = layers.Embedding(13,output_dim = int(12//1))(month_encoded) #divisor
    month_embedded = layers.Flatten()(month_embedded)

    # String categorical features
    flat_model_type_encoded = encode_categorical_feature2(flat_model_type, "flat_model_type", train_ds, True)
    flat_model_type_embedded = layers.Embedding(45,output_dim = floor(44//1))(flat_model_type_encoded) # divisor
    flat_model_type_embedded = layers.Flatten()(flat_model_type_embedded)
    storey_range_encoded = encode_categorical_feature2(storey_range, "storey_range", train_ds, True)
    storey_range_embedded = layers.Embedding(18,output_dim = floor(17//1))(storey_range_encoded) #divisor
    storey_range_embedded = layers.Flatten()(storey_range_embedded)

    # Numerical features
    dist_to_nearest_stn_encoded = encode_numerical_feature(dist_to_nearest_stn, "dist_to_nearest_stn", train_ds)
    dist_to_dhoby_encoded = encode_numerical_feature(dist_to_dhoby, "dist_to_dhoby", train_ds)
    degree_centrality_encoded = encode_numerical_feature(degree_centrality, "degree_centrality", train_ds)
    eigenvector_centrality_encoded = encode_numerical_feature(eigenvector_centrality, "eigenvector_centrality", train_ds)
    remaining_lease_years = encode_numerical_feature(remaining_lease_years, "remaining_lease_years", train_ds)
    floor_area_sqm_encoded = encode_numerical_feature(floor_area_sqm, "floor_area_sqm", train_ds)

    all_features = layers.concatenate(
        [
            month_encoded,
            #month_embedded,
            #flat_model_type_encoded,
            flat_model_type_embedded,
            #storey_range_encoded,
            storey_range_embedded,
            floor_area_sqm_encoded,
            remaining_lease_years,
            degree_centrality_encoded,
            eigenvector_centrality_encoded,
            dist_to_nearest_stn_encoded,
            dist_to_dhoby_encoded]
    )
    #emd = layers.Embedding(10,output_dim = floor(9/hp.Int('divisor',min_value=1,max_value=2,step=1)))(all_features)

    x = layers.Dense(units=64,activation='relu')(all_features) #units
    output = layers.Dense(1)(x)
    model = keras.Model(all_inputs, output)
    learning_rate = 0.04559624000604727 #lr
    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss="mean_squared_error", metrics=[keras.metrics.RootMeanSquaredError()])

    return model

#Build the model and compile
q2_model = q2d_build_model()

#Getting the summary of the model to visualize the model
q2_model.summary()

"""# **Qn 2**
d. Using the best model configuration, train a model on the non-test split for 50 epochs. Generate a plot to show how the train
and RMSE changes across epochs.
"""

# Split data

train_dataframe2 = temp_df_train # TODO
val_dataframe2 = temp_df_val # TODO

train_ds2 = dataframe_to_dataset(train_dataframe)
val_ds2 = dataframe_to_dataset(val_dataframe)

train_ds2 = train_ds2.batch(256)
val_ds2 = val_ds2.batch(256)

# Creating a Checkpoint callback function to save the best model during training, which can be used later to testing and
# Deploying purpose.
checkpoint2 = callbacks.ModelCheckpoint(
                        'q2d_model.tf',
                        verbose=1,
                        save_best_only=True,
                        monitor='val_loss')

#Training the model and Testing it to check the performance in every epoch
hist = q2_model.fit(train_ds2, epochs=50, validation_data=val_ds2, callbacks = [checkpoint2])

#Importing the Required liraries for plotting
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import metrics

plt.plot(hist.history['root_mean_squared_error'])
#plt.plot(hist.history['val_root_mean_squared_error'])
plt.title('model error')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(hist.history['val_root_mean_squared_error'])
plt.title('model error')
plt.ylabel('val_root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['test'], loc='upper left')
plt.show()

plt.plot(hist.history['loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train'], loc='upper left')
plt.show()

plt.plot(hist.history['val_loss'])
plt.title('model loss')
plt.ylabel('val_loss')
plt.xlabel('epoch')
plt.legend(['train'], loc='upper left')
plt.show()

"""# **Qn 2**
e. Using the model from the best epoch, report the test R2 value and show the 4 top 30 test samples with the largest errors. List down any trends you find in
these samples and suggest ways to reduce these errors.
"""

#Defining new Function for the purpose of meeting the requirements of testing the models
def dataframe_to_dataset_test(dataframe):
    dataframe = dataframe.copy()
    labels = dataframe.pop("resale_price")
    labels =tf.cast(labels,tf.int32)
    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe)))
    ds = ds.shuffle(buffer_size=len(dataframe))
    return ds

def dataframe_to_dataset_labels(dataframe):
    dataframe = dataframe.copy()
    labels = dataframe.pop("resale_price")
    labels =tf.cast(labels,tf.int32)
    ds = tf.data.Dataset.from_tensor_slices((labels))
    ds = ds.shuffle(buffer_size=len(dataframe))
    return ds

#Creating the testing data set for predicting
val_ds_test = dataframe_to_dataset(val_dataframe)
val_ds_test = val_ds_test.batch(256)

#Predicting the test data
y_pred = q2_model.predict(val_ds_test)

# Predicted data
y_pred

y_pred = y_pred.reshape(-1)

y_true = val_dataframe.pop("resale_price")

y_true.values.reshape

difference = y_true.values - y_pred

sorted_index_array = np.argsort(difference)
sorted_array = difference[sorted_index_array]
rslt = sorted_array[-30 : ]

#Top 30 error in the prediction compared to original data. (Descending order)
rslt

#Craeting a New Dataset for the question 3
old_df = pd.read_csv('/content/drive/MyDrive/hdb_price_prediction_old.csv')
old_df

old_test_set = old_df[old_df['year']==2021]

old_test_set = old_test_set[['dist_to_nearest_stn', 'dist_to_dhoby',  'degree_centrality', 'eigenvector_centrality',
                  'remaining_lease_years', 'floor_area_sqm', 'month', 'flat_model_type', 'storey_range', 'resale_price']]

new_df = pd.read_csv('hdb_price_prediction.csv')
new_df

new_test_set = new_df[new_df['year'] >= 2021]

new_test_set = new_test_set[['dist_to_nearest_stn', 'dist_to_dhoby',  'degree_centrality', 'eigenvector_centrality',
                  'remaining_lease_years', 'floor_area_sqm', 'month', 'flat_model_type', 'storey_range', 'resale_price']]

#Defining new functions to avoid shuffle to generate similar result during testing
def dataframe_to_dataset_test_unshuffle(dataframe):
    dataframe = dataframe.copy()
    labels = dataframe.pop("resale_price")
    labels =tf.cast(labels,tf.int32)
    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe)))
    return ds

def dataframe_to_dataset_labels_unshuffle(dataframe):
    dataframe = dataframe.copy()
    labels = dataframe.pop("resale_price")
    return labels

# Split data

old_test_set_ds = dataframe_to_dataset_test_unshuffle(old_test_set)
new_test_set_ds = dataframe_to_dataset_test_unshuffle(new_test_set)

old_test_set_ds_label = dataframe_to_dataset_labels_unshuffle(old_test_set)
new_test_set_ds_label = dataframe_to_dataset_labels_unshuffle(new_test_set)


old_test_set_ds = old_test_set_ds.batch(256)
new_test_set_ds = new_test_set_ds.batch(256)

"""# **Qn 3**
a.  Apply your model from Q2d on the ‘old test set’. On the ‘new test set’, split it into 2021 and 2022. For all 3 test sets, report the test R2 value you obtained.
"""

# Creating a new dataset for part a - requirement
new_test_set_1 = new_df[new_df['year'] == 2021]
new_test_set_2 = new_df[new_df['year'] == 2022]

new_test_set_1 = new_test_set_1[['dist_to_nearest_stn', 'dist_to_dhoby',  'degree_centrality', 'eigenvector_centrality',
                  'remaining_lease_years', 'floor_area_sqm', 'month', 'flat_model_type', 'storey_range', 'resale_price']]
new_test_set_2 = new_test_set_2[['dist_to_nearest_stn', 'dist_to_dhoby',  'degree_centrality', 'eigenvector_centrality',
                  'remaining_lease_years', 'floor_area_sqm', 'month', 'flat_model_type', 'storey_range', 'resale_price']]

new_test_set_1_ds = dataframe_to_dataset_test_unshuffle(new_test_set_1)
new_test_set_2_ds = dataframe_to_dataset_test_unshuffle(new_test_set_2)

new_test_set_1_ds_label = dataframe_to_dataset_labels_unshuffle(new_test_set_1)
new_test_set_2_ds_label = dataframe_to_dataset_labels_unshuffle(new_test_set_2)

new_test_set_1_ds = new_test_set_1_ds.batch(256)
new_test_set_2_ds = new_test_set_2_ds.batch(256)

# Prediction for old test set
y_pred_old_test_set = q2_model.predict(old_test_set_ds)

print(y_pred_old_test_set.reshape(-1), old_test_set_ds_label.values)

# Prediction for new test set 1
y_pred_new_test_set_1 = q2_model.predict(new_test_set_1_ds)

print(y_pred_new_test_set_1.reshape(-1), new_test_set_1_ds_label.values)

# Prediction for new test set 2
y_pred_new_test_set_2 = q2_model.predict(new_test_set_2_ds)

print(y_pred_new_test_set_2.reshape(-1), new_test_set_2_ds_label.values)

from sklearn.metrics import r2_score
#libraries for R2 score, was unable to use r2 previously defined as it required in tensor format

print('Old Test Set R2 value:',r2_score(old_test_set_ds_label.values,y_pred_old_test_set))

print('New Test Set 1 R2 value:',r2_score(new_test_set_1_ds_label.values,y_pred_new_test_set_1))

print('New Test Set 2 R2 value:',r2_score(new_test_set_2_ds_label.values,y_pred_new_test_set_2))

"""# **Qn 3**
b. In light of this (along with their result in Q1b and your results from Q3a), compare the extent to which model degradation has impacted your model to that of the team’s linear regression model and explain why this has occurred

There is significant model degradation that is happening as the R^2 value of the intial model in Q1b is 0.626 and using Sklearn it was 0.627, from that it has decreased to 0.465 for neural network and 0.464 for Sklearn. But as you can see the model degrdation has been the same for both the models.

Model Degradation mainly happens as a result of shift in trend i.e once the model is trained and deployed we are not retraining the model, so if there is slight shift in trend from the original training dataset the model performs poorly

# **Qn 3**
c.
With appropriate plots, visualise the distributions of all the features and labels used by the model. Which variable(s) showed the largest covariate/label shift that might have led to the drop in model performance as seen in Q3b?
"""

#Importing Libraries for better plots
import seaborn as sns

sns.set_theme(style="darkgrid") # for better vizualization

new_df_train = new_df[new_df['year'] < 2021] # new dataset for part c requirements

# Creating a Kdeplot
fig = plt.figure(figsize=(15,5))
ax = fig.add_subplot(111)
sns.kdeplot(data=new_df_train , x='dist_to_nearest_stn', ax=ax)
sns.kdeplot(data=new_test_set_2 , x='dist_to_nearest_stn', ax=ax)
plt.show()

# Creating a Kdeplot
fig = plt.figure(figsize=(15,5))
ax = fig.add_subplot(111)
sns.kdeplot(data=new_df_train , x='dist_to_dhoby', ax=ax)
sns.kdeplot(data=new_test_set_2 , x='dist_to_dhoby', ax=ax)
plt.show()

# Creating a Kdeplot
fig = plt.figure(figsize=(15,5))
ax = fig.add_subplot(111)
sns.kdeplot(data=new_df_train , x='degree_centrality', ax=ax)
sns.kdeplot(data=new_test_set_2 , x='degree_centrality', ax=ax)
plt.show()

# Creating a Kdeplot
fig = plt.figure(figsize=(15,5))
ax = fig.add_subplot(111)
sns.kdeplot(data=new_df_train , x='eigenvector_centrality', ax=ax)
sns.kdeplot(data=new_test_set_2 , x='eigenvector_centrality', ax=ax)
plt.show()

# Creating a Kdeplot
fig = plt.figure(figsize=(15,5))
ax = fig.add_subplot(111)
sns.kdeplot(data=new_df_train , x='remaining_lease_years', ax=ax, label='Training')
sns.kdeplot(data=new_test_set_2 , x='remaining_lease_years', ax=ax, label='Testing(2022)')
ax.legend(loc="best")
plt.show()

#There is significant shift in the dataset of year training(<2021) and 2022
#Largest Shift in feature

# Creating a Kdeplot
fig = plt.figure(figsize=(15,5))
ax = fig.add_subplot(111)
sns.kdeplot(data=new_df_train , x='floor_area_sqm', ax=ax)
sns.kdeplot(data=new_test_set_2 , x='floor_area_sqm', ax=ax)
plt.show()

# Creating a Histplot
fig = plt.figure(figsize=(15,5))
ax = fig.add_subplot(111)
sns.kdeplot(data=new_df_train , x='month', ax=ax, label='Training')
sns.kdeplot(data=new_test_set_2 , x='month', ax=ax, label='Testing(2022)')
ax.legend(loc="best")
plt.show()
#Lack of data after months 8

fig = plt.figure(figsize=(15,5))
ax = fig.add_subplot(111)
sns.histplot(data=new_df_train , x='flat_model_type', ax=ax, kde=True)
sns.histplot(data=new_test_set_2 , x='flat_model_type', ax=ax, kde=True)
plt.setp(ax.get_xticklabels(), rotation=90)
plt.show()

fig = plt.figure(figsize=(15,5))
ax = fig.add_subplot(111)
sns.histplot(data=new_df_train , x='storey_range', ax=ax, kde=True, stat='density')
sns.histplot(data=new_test_set_2 , x='storey_range', ax=ax, kde=True, stat='density')
plt.setp(ax.get_xticklabels(), rotation=90)
plt.show()

fig = plt.figure(figsize=(15,5))
ax = fig.add_subplot(111)
sns.regplot(data=new_df_train , x='remaining_lease_years', y='resale_price', ax=ax, scatter_kws={'s':1}, label='Training')
sns.regplot(data=new_test_set_2 , x='remaining_lease_years', y='resale_price' , ax=ax, scatter_kws={'s':1}, label='Testing(2022)')
ax.legend(loc="best")
plt.setp(ax.get_xticklabels(), rotation=90)
plt.show()
#It is Clearly Visible that the there is shift in Data, indicated by the shift in linear regression of Training and Testing

fig = plt.figure(figsize=(15,5))
ax = fig.add_subplot(111)
sns.regplot(data=new_df_train , x='dist_to_dhoby', y='resale_price', ax=ax, scatter_kws={'s':1}, label='Training')
sns.regplot(data=new_test_set_2 , x='dist_to_dhoby', y='resale_price' , ax=ax, scatter_kws={'s':1}, label='Testing(2022)')
ax.legend(loc="best")
plt.setp(ax.get_xticklabels(), rotation=90)
plt.show()

fig = plt.figure(figsize=(15,5))
ax = fig.add_subplot(111)
sns.regplot(data=new_df_train , x='floor_area_sqm', y='resale_price', ax=ax, scatter_kws={'s':1}, label='Training')
sns.regplot(data=new_test_set_2 , x='floor_area_sqm', y='resale_price' , ax=ax, scatter_kws={'s':1}, label='Testing(2022)')
ax.legend(loc="best")
plt.setp(ax.get_xticklabels(), rotation=90)
plt.show()

fig = plt.figure(figsize=(15,5))
ax = fig.add_subplot(111)
sns.regplot(data=new_df_train , x='dist_to_nearest_stn', y='resale_price', ax=ax, scatter_kws={'s':1}, label='Training')
sns.regplot(data=new_test_set_2 , x='dist_to_nearest_stn', y='resale_price' , ax=ax, scatter_kws={'s':1}, label='Testing(2022)')
ax.legend(loc="best")
plt.setp(ax.get_xticklabels(), rotation=90)
plt.show()

"""In general there is shift in the intercept for majority of the Features when comparing the Training and Testing(2022) data, This results in increased error by the model, which is shown in the form of reduced R2 score.

Feature selection also contributes to model degeradation as there could be less significant feature and features than could be more significant. If we remove the less significant features the model will be able to perform better on new test data therby avoiding model degradation.

# **Qn 3**
d. The team passed you a script (‘RFE.py’) that recursively removes features 2 from a neural network, so as to find the best feature subset. Run this piece of
code with your model from Q2d and report the best feature subset obtained.
"""

# Commented out IPython magic to ensure Python compatibility.
# %run RFE

vec #Best feature Subset Obtained
#So eliminiated Feature eigenvector_centrality
# This might change for Each run

"""Final Output Reviced - [52998.65625, 56539.4375, 58515.94921875, 59919.23828125, 75327.6171875, 54313.859375, 52262.8046875, 64286.40625, 56762.171875]

# **Qn 3**
e. RFE on the ‘old test set’ eliminated features degree_centrality and month. It 3 also showed that dist_to_dhoby and dist_to_nearest_stn are crucial.

Compare these features to those in Q3d and
discuss whether concept drift has occurred.
"""

# Plotting for check for shift in data from old test set to new test set
fig = plt.figure(figsize=(15,5))
ax = fig.add_subplot(111)
sns.regplot(data=old_test_set , x='dist_to_nearest_stn', y='resale_price', ax=ax, scatter_kws={'s':1}, label='Testing(old)')
sns.regplot(data=new_test_set_2 , x='dist_to_nearest_stn', y='resale_price' , ax=ax, scatter_kws={'s':1}, label='Testing(new)')
ax.legend(loc="best")
plt.setp(ax.get_xticklabels(), rotation=90)
plt.show()

# Plotting for check for shift in data from old test set to new test set
fig = plt.figure(figsize=(15,5))
ax = fig.add_subplot(111)
sns.regplot(data=old_test_set , x='dist_to_dhoby', y='resale_price', ax=ax, scatter_kws={'s':1}, label='Testing(old)')
sns.regplot(data=new_test_set_2 , x='dist_to_dhoby', y='resale_price' , ax=ax, scatter_kws={'s':1}, label='Testing(new)')
ax.legend(loc="best")
plt.setp(ax.get_xticklabels(), rotation=90)
plt.show()

"""It is quite visible that there is shift in dataset when comparing the old test set to new test set from the fact that two linear regression line though parallel, there is an increase in the value of the property for the same X value. i.e. In 2021 if dist_to_dhoby was 10, resale price would have been - 0.54e6, but in 2022 it would be 0.58e6

This shift is quite common for house prices, as there is general appreciation in value of house asset class every year in a growing economy.
"""

# Plotting for check for shift in data from old test set to new test set
fig = plt.figure(figsize=(15,5))
ax = fig.add_subplot(111)
sns.scatterplot(data=old_test_set , x='month', y='resale_price', ax=ax, label='Testing(old)')
sns.scatterplot(data=new_test_set_2 , x='month', y='resale_price' , ax=ax, label='Testing(new)')
ax.legend(loc="best")
plt.setp(ax.get_xticklabels(), rotation=90)
plt.show()

# Plotting for check for shift in data from old test set to new test set
fig = plt.figure(figsize=(15,5))
ax = fig.add_subplot(111)
sns.scatterplot(data=old_test_set , x='degree_centrality', y='resale_price', ax=ax, label='Testing(old)')
sns.scatterplot(data=new_test_set_2 , x='degree_centrality', y='resale_price' , ax=ax, label='Testing(new)')
ax.legend(loc="best")
plt.setp(ax.get_xticklabels(), rotation=90)
plt.show()

"""In the Case of features Month and Degree Centrality, it visisble that there is no significant change or shift in dataset, so by removing these features the model performance might not be affected by a big margin."""